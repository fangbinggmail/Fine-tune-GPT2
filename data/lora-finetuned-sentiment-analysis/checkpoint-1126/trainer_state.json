{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1126,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017761989342806393,
      "grad_norm": 1.686641812324524,
      "learning_rate": 9.920071047957373e-06,
      "loss": 0.717,
      "step": 10
    },
    {
      "epoch": 0.035523978685612786,
      "grad_norm": 2.9435641765594482,
      "learning_rate": 9.83126110124334e-06,
      "loss": 0.6913,
      "step": 20
    },
    {
      "epoch": 0.05328596802841918,
      "grad_norm": 5.825891017913818,
      "learning_rate": 9.742451154529307e-06,
      "loss": 0.7035,
      "step": 30
    },
    {
      "epoch": 0.07104795737122557,
      "grad_norm": 1.6128360033035278,
      "learning_rate": 9.653641207815277e-06,
      "loss": 0.7092,
      "step": 40
    },
    {
      "epoch": 0.08880994671403197,
      "grad_norm": 2.2270638942718506,
      "learning_rate": 9.564831261101243e-06,
      "loss": 0.7192,
      "step": 50
    },
    {
      "epoch": 0.10657193605683836,
      "grad_norm": 4.471869468688965,
      "learning_rate": 9.476021314387213e-06,
      "loss": 0.704,
      "step": 60
    },
    {
      "epoch": 0.12433392539964476,
      "grad_norm": 2.7803308963775635,
      "learning_rate": 9.38721136767318e-06,
      "loss": 0.7055,
      "step": 70
    },
    {
      "epoch": 0.14209591474245115,
      "grad_norm": 2.4496402740478516,
      "learning_rate": 9.298401420959147e-06,
      "loss": 0.7104,
      "step": 80
    },
    {
      "epoch": 0.15985790408525755,
      "grad_norm": 2.950899839401245,
      "learning_rate": 9.209591474245117e-06,
      "loss": 0.7004,
      "step": 90
    },
    {
      "epoch": 0.17761989342806395,
      "grad_norm": 1.6209313869476318,
      "learning_rate": 9.120781527531083e-06,
      "loss": 0.7111,
      "step": 100
    },
    {
      "epoch": 0.19538188277087035,
      "grad_norm": 2.7947206497192383,
      "learning_rate": 9.031971580817053e-06,
      "loss": 0.7015,
      "step": 110
    },
    {
      "epoch": 0.21314387211367672,
      "grad_norm": 3.517000913619995,
      "learning_rate": 8.943161634103021e-06,
      "loss": 0.6763,
      "step": 120
    },
    {
      "epoch": 0.23090586145648312,
      "grad_norm": 1.7054325342178345,
      "learning_rate": 8.854351687388987e-06,
      "loss": 0.7044,
      "step": 130
    },
    {
      "epoch": 0.24866785079928952,
      "grad_norm": 3.158808469772339,
      "learning_rate": 8.765541740674957e-06,
      "loss": 0.7087,
      "step": 140
    },
    {
      "epoch": 0.2664298401420959,
      "grad_norm": 1.640280842781067,
      "learning_rate": 8.676731793960925e-06,
      "loss": 0.7114,
      "step": 150
    },
    {
      "epoch": 0.2841918294849023,
      "grad_norm": 5.226465702056885,
      "learning_rate": 8.587921847246893e-06,
      "loss": 0.7087,
      "step": 160
    },
    {
      "epoch": 0.3019538188277087,
      "grad_norm": 2.814844846725464,
      "learning_rate": 8.499111900532861e-06,
      "loss": 0.6986,
      "step": 170
    },
    {
      "epoch": 0.3197158081705151,
      "grad_norm": 3.9679882526397705,
      "learning_rate": 8.410301953818827e-06,
      "loss": 0.7047,
      "step": 180
    },
    {
      "epoch": 0.33747779751332146,
      "grad_norm": 3.046933174133301,
      "learning_rate": 8.321492007104797e-06,
      "loss": 0.703,
      "step": 190
    },
    {
      "epoch": 0.3552397868561279,
      "grad_norm": 8.719998359680176,
      "learning_rate": 8.232682060390765e-06,
      "loss": 0.695,
      "step": 200
    },
    {
      "epoch": 0.37300177619893427,
      "grad_norm": 3.2065012454986572,
      "learning_rate": 8.143872113676733e-06,
      "loss": 0.6837,
      "step": 210
    },
    {
      "epoch": 0.3907637655417407,
      "grad_norm": 2.668394088745117,
      "learning_rate": 8.0550621669627e-06,
      "loss": 0.7023,
      "step": 220
    },
    {
      "epoch": 0.40852575488454707,
      "grad_norm": 3.3439407348632812,
      "learning_rate": 7.966252220248669e-06,
      "loss": 0.6986,
      "step": 230
    },
    {
      "epoch": 0.42628774422735344,
      "grad_norm": 5.422231197357178,
      "learning_rate": 7.877442273534637e-06,
      "loss": 0.6823,
      "step": 240
    },
    {
      "epoch": 0.44404973357015987,
      "grad_norm": 1.7539699077606201,
      "learning_rate": 7.788632326820605e-06,
      "loss": 0.7019,
      "step": 250
    },
    {
      "epoch": 0.46181172291296624,
      "grad_norm": 8.230183601379395,
      "learning_rate": 7.699822380106573e-06,
      "loss": 0.7001,
      "step": 260
    },
    {
      "epoch": 0.47957371225577267,
      "grad_norm": 3.441861391067505,
      "learning_rate": 7.611012433392541e-06,
      "loss": 0.6961,
      "step": 270
    },
    {
      "epoch": 0.49733570159857904,
      "grad_norm": 1.6077286005020142,
      "learning_rate": 7.522202486678508e-06,
      "loss": 0.7048,
      "step": 280
    },
    {
      "epoch": 0.5150976909413855,
      "grad_norm": 2.51346492767334,
      "learning_rate": 7.433392539964477e-06,
      "loss": 0.6987,
      "step": 290
    },
    {
      "epoch": 0.5328596802841918,
      "grad_norm": 3.815297842025757,
      "learning_rate": 7.344582593250445e-06,
      "loss": 0.6885,
      "step": 300
    },
    {
      "epoch": 0.5506216696269982,
      "grad_norm": 5.465555667877197,
      "learning_rate": 7.255772646536413e-06,
      "loss": 0.6933,
      "step": 310
    },
    {
      "epoch": 0.5683836589698046,
      "grad_norm": 2.442544937133789,
      "learning_rate": 7.1669626998223805e-06,
      "loss": 0.7088,
      "step": 320
    },
    {
      "epoch": 0.5861456483126111,
      "grad_norm": 3.23626971244812,
      "learning_rate": 7.0781527531083485e-06,
      "loss": 0.6909,
      "step": 330
    },
    {
      "epoch": 0.6039076376554174,
      "grad_norm": 3.754296064376831,
      "learning_rate": 6.989342806394317e-06,
      "loss": 0.7107,
      "step": 340
    },
    {
      "epoch": 0.6216696269982238,
      "grad_norm": 3.244699239730835,
      "learning_rate": 6.9005328596802845e-06,
      "loss": 0.7156,
      "step": 350
    },
    {
      "epoch": 0.6394316163410302,
      "grad_norm": 7.474362850189209,
      "learning_rate": 6.811722912966253e-06,
      "loss": 0.6971,
      "step": 360
    },
    {
      "epoch": 0.6571936056838366,
      "grad_norm": 1.819866418838501,
      "learning_rate": 6.7229129662522204e-06,
      "loss": 0.6939,
      "step": 370
    },
    {
      "epoch": 0.6749555950266429,
      "grad_norm": 2.352750778198242,
      "learning_rate": 6.634103019538188e-06,
      "loss": 0.6941,
      "step": 380
    },
    {
      "epoch": 0.6927175843694494,
      "grad_norm": 4.1663289070129395,
      "learning_rate": 6.545293072824157e-06,
      "loss": 0.7032,
      "step": 390
    },
    {
      "epoch": 0.7104795737122558,
      "grad_norm": 2.57773494720459,
      "learning_rate": 6.456483126110124e-06,
      "loss": 0.7016,
      "step": 400
    },
    {
      "epoch": 0.7282415630550622,
      "grad_norm": 5.718743801116943,
      "learning_rate": 6.367673179396093e-06,
      "loss": 0.6945,
      "step": 410
    },
    {
      "epoch": 0.7460035523978685,
      "grad_norm": 5.596828937530518,
      "learning_rate": 6.278863232682061e-06,
      "loss": 0.7015,
      "step": 420
    },
    {
      "epoch": 0.7637655417406749,
      "grad_norm": 3.2233338356018066,
      "learning_rate": 6.190053285968028e-06,
      "loss": 0.6869,
      "step": 430
    },
    {
      "epoch": 0.7815275310834814,
      "grad_norm": 3.134434700012207,
      "learning_rate": 6.101243339253997e-06,
      "loss": 0.7019,
      "step": 440
    },
    {
      "epoch": 0.7992895204262878,
      "grad_norm": 1.5405457019805908,
      "learning_rate": 6.012433392539965e-06,
      "loss": 0.6845,
      "step": 450
    },
    {
      "epoch": 0.8170515097690941,
      "grad_norm": 2.0745556354522705,
      "learning_rate": 5.923623445825933e-06,
      "loss": 0.6889,
      "step": 460
    },
    {
      "epoch": 0.8348134991119005,
      "grad_norm": 1.579626202583313,
      "learning_rate": 5.834813499111901e-06,
      "loss": 0.6927,
      "step": 470
    },
    {
      "epoch": 0.8525754884547069,
      "grad_norm": 5.849127292633057,
      "learning_rate": 5.746003552397868e-06,
      "loss": 0.6965,
      "step": 480
    },
    {
      "epoch": 0.8703374777975134,
      "grad_norm": 3.259244680404663,
      "learning_rate": 5.657193605683837e-06,
      "loss": 0.6901,
      "step": 490
    },
    {
      "epoch": 0.8880994671403197,
      "grad_norm": 8.315437316894531,
      "learning_rate": 5.568383658969805e-06,
      "loss": 0.6968,
      "step": 500
    },
    {
      "epoch": 0.9058614564831261,
      "grad_norm": 3.2119224071502686,
      "learning_rate": 5.479573712255774e-06,
      "loss": 0.7105,
      "step": 510
    },
    {
      "epoch": 0.9236234458259325,
      "grad_norm": 1.5669828653335571,
      "learning_rate": 5.390763765541741e-06,
      "loss": 0.6895,
      "step": 520
    },
    {
      "epoch": 0.9413854351687388,
      "grad_norm": 5.082596302032471,
      "learning_rate": 5.301953818827709e-06,
      "loss": 0.685,
      "step": 530
    },
    {
      "epoch": 0.9591474245115453,
      "grad_norm": 2.7566404342651367,
      "learning_rate": 5.213143872113677e-06,
      "loss": 0.7164,
      "step": 540
    },
    {
      "epoch": 0.9769094138543517,
      "grad_norm": 5.024191379547119,
      "learning_rate": 5.124333925399645e-06,
      "loss": 0.6968,
      "step": 550
    },
    {
      "epoch": 0.9946714031971581,
      "grad_norm": 7.1140546798706055,
      "learning_rate": 5.035523978685614e-06,
      "loss": 0.7097,
      "step": 560
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.5196636434073195,
      "eval_loss": 0.6902143955230713,
      "eval_precision": 0.5501194797338174,
      "eval_recall": 0.538,
      "eval_runtime": 95.4719,
      "eval_samples_per_second": 5.237,
      "eval_steps_per_second": 0.66,
      "step": 563
    },
    {
      "epoch": 1.0124333925399644,
      "grad_norm": 3.354565382003784,
      "learning_rate": 4.946714031971581e-06,
      "loss": 0.7087,
      "step": 570
    },
    {
      "epoch": 1.030195381882771,
      "grad_norm": 2.933387279510498,
      "learning_rate": 4.85790408525755e-06,
      "loss": 0.6896,
      "step": 580
    },
    {
      "epoch": 1.0479573712255772,
      "grad_norm": 1.5526965856552124,
      "learning_rate": 4.769094138543518e-06,
      "loss": 0.7061,
      "step": 590
    },
    {
      "epoch": 1.0657193605683837,
      "grad_norm": 1.8414113521575928,
      "learning_rate": 4.680284191829485e-06,
      "loss": 0.6917,
      "step": 600
    },
    {
      "epoch": 1.0834813499111902,
      "grad_norm": 6.573897361755371,
      "learning_rate": 4.591474245115453e-06,
      "loss": 0.6977,
      "step": 610
    },
    {
      "epoch": 1.1012433392539964,
      "grad_norm": 1.739477515220642,
      "learning_rate": 4.502664298401422e-06,
      "loss": 0.6898,
      "step": 620
    },
    {
      "epoch": 1.119005328596803,
      "grad_norm": 1.6434987783432007,
      "learning_rate": 4.41385435168739e-06,
      "loss": 0.7042,
      "step": 630
    },
    {
      "epoch": 1.1367673179396092,
      "grad_norm": 5.102304935455322,
      "learning_rate": 4.3250444049733576e-06,
      "loss": 0.6989,
      "step": 640
    },
    {
      "epoch": 1.1545293072824157,
      "grad_norm": 8.245373725891113,
      "learning_rate": 4.2362344582593255e-06,
      "loss": 0.7007,
      "step": 650
    },
    {
      "epoch": 1.1722912966252221,
      "grad_norm": 2.636350154876709,
      "learning_rate": 4.1474245115452935e-06,
      "loss": 0.6936,
      "step": 660
    },
    {
      "epoch": 1.1900532859680284,
      "grad_norm": 2.6078977584838867,
      "learning_rate": 4.0586145648312615e-06,
      "loss": 0.7026,
      "step": 670
    },
    {
      "epoch": 1.2078152753108349,
      "grad_norm": 3.109293222427368,
      "learning_rate": 3.9698046181172295e-06,
      "loss": 0.7001,
      "step": 680
    },
    {
      "epoch": 1.2255772646536411,
      "grad_norm": 2.5961320400238037,
      "learning_rate": 3.8809946714031975e-06,
      "loss": 0.6986,
      "step": 690
    },
    {
      "epoch": 1.2433392539964476,
      "grad_norm": 5.557217121124268,
      "learning_rate": 3.792184724689166e-06,
      "loss": 0.6739,
      "step": 700
    },
    {
      "epoch": 1.261101243339254,
      "grad_norm": 3.130737781524658,
      "learning_rate": 3.7033747779751334e-06,
      "loss": 0.6929,
      "step": 710
    },
    {
      "epoch": 1.2788632326820604,
      "grad_norm": 3.371814012527466,
      "learning_rate": 3.6145648312611014e-06,
      "loss": 0.6931,
      "step": 720
    },
    {
      "epoch": 1.2966252220248669,
      "grad_norm": 1.7597423791885376,
      "learning_rate": 3.52575488454707e-06,
      "loss": 0.6946,
      "step": 730
    },
    {
      "epoch": 1.3143872113676731,
      "grad_norm": 2.686990737915039,
      "learning_rate": 3.4369449378330378e-06,
      "loss": 0.6994,
      "step": 740
    },
    {
      "epoch": 1.3321492007104796,
      "grad_norm": 3.6390042304992676,
      "learning_rate": 3.3481349911190058e-06,
      "loss": 0.7046,
      "step": 750
    },
    {
      "epoch": 1.349911190053286,
      "grad_norm": 2.9417002201080322,
      "learning_rate": 3.2593250444049733e-06,
      "loss": 0.6938,
      "step": 760
    },
    {
      "epoch": 1.3676731793960923,
      "grad_norm": 3.0799496173858643,
      "learning_rate": 3.1705150976909417e-06,
      "loss": 0.6836,
      "step": 770
    },
    {
      "epoch": 1.3854351687388988,
      "grad_norm": 1.9484176635742188,
      "learning_rate": 3.0817051509769097e-06,
      "loss": 0.6999,
      "step": 780
    },
    {
      "epoch": 1.403197158081705,
      "grad_norm": 2.5281784534454346,
      "learning_rate": 2.9928952042628777e-06,
      "loss": 0.7016,
      "step": 790
    },
    {
      "epoch": 1.4209591474245116,
      "grad_norm": 3.3512215614318848,
      "learning_rate": 2.904085257548846e-06,
      "loss": 0.7215,
      "step": 800
    },
    {
      "epoch": 1.438721136767318,
      "grad_norm": 8.258833885192871,
      "learning_rate": 2.8152753108348136e-06,
      "loss": 0.695,
      "step": 810
    },
    {
      "epoch": 1.4564831261101243,
      "grad_norm": 2.3678641319274902,
      "learning_rate": 2.7264653641207816e-06,
      "loss": 0.6882,
      "step": 820
    },
    {
      "epoch": 1.4742451154529308,
      "grad_norm": 1.5577791929244995,
      "learning_rate": 2.63765541740675e-06,
      "loss": 0.6893,
      "step": 830
    },
    {
      "epoch": 1.492007104795737,
      "grad_norm": 3.2832531929016113,
      "learning_rate": 2.548845470692718e-06,
      "loss": 0.6978,
      "step": 840
    },
    {
      "epoch": 1.5097690941385435,
      "grad_norm": 2.8433890342712402,
      "learning_rate": 2.460035523978686e-06,
      "loss": 0.6825,
      "step": 850
    },
    {
      "epoch": 1.52753108348135,
      "grad_norm": 5.457941055297852,
      "learning_rate": 2.371225577264654e-06,
      "loss": 0.696,
      "step": 860
    },
    {
      "epoch": 1.5452930728241563,
      "grad_norm": 2.9581377506256104,
      "learning_rate": 2.282415630550622e-06,
      "loss": 0.7039,
      "step": 870
    },
    {
      "epoch": 1.5630550621669625,
      "grad_norm": 1.672402024269104,
      "learning_rate": 2.19360568383659e-06,
      "loss": 0.6945,
      "step": 880
    },
    {
      "epoch": 1.580817051509769,
      "grad_norm": 1.6523276567459106,
      "learning_rate": 2.104795737122558e-06,
      "loss": 0.7018,
      "step": 890
    },
    {
      "epoch": 1.5985790408525755,
      "grad_norm": 1.6480669975280762,
      "learning_rate": 2.015985790408526e-06,
      "loss": 0.7099,
      "step": 900
    },
    {
      "epoch": 1.616341030195382,
      "grad_norm": 1.9329311847686768,
      "learning_rate": 1.927175843694494e-06,
      "loss": 0.6978,
      "step": 910
    },
    {
      "epoch": 1.6341030195381883,
      "grad_norm": 3.0089685916900635,
      "learning_rate": 1.8383658969804618e-06,
      "loss": 0.6982,
      "step": 920
    },
    {
      "epoch": 1.6518650088809945,
      "grad_norm": 3.206824541091919,
      "learning_rate": 1.74955595026643e-06,
      "loss": 0.7129,
      "step": 930
    },
    {
      "epoch": 1.669626998223801,
      "grad_norm": 6.177987098693848,
      "learning_rate": 1.6607460035523982e-06,
      "loss": 0.7013,
      "step": 940
    },
    {
      "epoch": 1.6873889875666075,
      "grad_norm": 3.568211078643799,
      "learning_rate": 1.571936056838366e-06,
      "loss": 0.6983,
      "step": 950
    },
    {
      "epoch": 1.705150976909414,
      "grad_norm": 3.3783316612243652,
      "learning_rate": 1.4831261101243342e-06,
      "loss": 0.6989,
      "step": 960
    },
    {
      "epoch": 1.7229129662522202,
      "grad_norm": 7.584776401519775,
      "learning_rate": 1.394316163410302e-06,
      "loss": 0.6853,
      "step": 970
    },
    {
      "epoch": 1.7406749555950265,
      "grad_norm": 5.155040264129639,
      "learning_rate": 1.3055062166962701e-06,
      "loss": 0.6791,
      "step": 980
    },
    {
      "epoch": 1.758436944937833,
      "grad_norm": 4.931034564971924,
      "learning_rate": 1.216696269982238e-06,
      "loss": 0.6958,
      "step": 990
    },
    {
      "epoch": 1.7761989342806395,
      "grad_norm": 3.817369222640991,
      "learning_rate": 1.127886323268206e-06,
      "loss": 0.6944,
      "step": 1000
    },
    {
      "epoch": 1.793960923623446,
      "grad_norm": 4.720638275146484,
      "learning_rate": 1.0390763765541743e-06,
      "loss": 0.6907,
      "step": 1010
    },
    {
      "epoch": 1.8117229129662522,
      "grad_norm": 3.886211395263672,
      "learning_rate": 9.502664298401422e-07,
      "loss": 0.6881,
      "step": 1020
    },
    {
      "epoch": 1.8294849023090585,
      "grad_norm": 5.405261993408203,
      "learning_rate": 8.614564831261102e-07,
      "loss": 0.7105,
      "step": 1030
    },
    {
      "epoch": 1.847246891651865,
      "grad_norm": 2.6212785243988037,
      "learning_rate": 7.726465364120782e-07,
      "loss": 0.6787,
      "step": 1040
    },
    {
      "epoch": 1.8650088809946714,
      "grad_norm": 1.6614129543304443,
      "learning_rate": 6.838365896980462e-07,
      "loss": 0.7028,
      "step": 1050
    },
    {
      "epoch": 1.882770870337478,
      "grad_norm": 1.5251320600509644,
      "learning_rate": 5.950266429840143e-07,
      "loss": 0.6832,
      "step": 1060
    },
    {
      "epoch": 1.9005328596802842,
      "grad_norm": 2.4670345783233643,
      "learning_rate": 5.062166962699822e-07,
      "loss": 0.6985,
      "step": 1070
    },
    {
      "epoch": 1.9182948490230904,
      "grad_norm": 5.221214294433594,
      "learning_rate": 4.174067495559503e-07,
      "loss": 0.7036,
      "step": 1080
    },
    {
      "epoch": 1.936056838365897,
      "grad_norm": 1.5781899690628052,
      "learning_rate": 3.2859680284191836e-07,
      "loss": 0.6975,
      "step": 1090
    },
    {
      "epoch": 1.9538188277087034,
      "grad_norm": 5.776216983795166,
      "learning_rate": 2.3978685612788634e-07,
      "loss": 0.7147,
      "step": 1100
    },
    {
      "epoch": 1.97158081705151,
      "grad_norm": 7.358647346496582,
      "learning_rate": 1.5097690941385437e-07,
      "loss": 0.698,
      "step": 1110
    },
    {
      "epoch": 1.9893428063943162,
      "grad_norm": 1.5694937705993652,
      "learning_rate": 6.216696269982238e-08,
      "loss": 0.7066,
      "step": 1120
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.5255887306691166,
      "eval_loss": 0.6877377033233643,
      "eval_precision": 0.5833615837484636,
      "eval_recall": 0.556,
      "eval_runtime": 96.4081,
      "eval_samples_per_second": 5.186,
      "eval_steps_per_second": 0.653,
      "step": 1126
    }
  ],
  "logging_steps": 10,
  "max_steps": 1126,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2107817639208960.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
