# Fine-tune-GPT2

## Introduction

This REPO contains a Jupyter notebook to fine tune the distilled GPT2 LLM model *distilgpt2* with data from the dataset *"talkmap/banking-conversation-corpus"*. We used only the forst 5000 entries to save the training time.

## Installation

The required packages are listed in the file
> requirements.txt

and it can be satisfied by
> pip install -r requirements.txt

preferably in a virtual environment.

## Implementation

The implementation of the fine tuning is in the self-explainatory Jupyter notebook
> LightweightFineTuning.ipynb


