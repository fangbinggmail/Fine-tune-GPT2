{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "Load pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f115fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.pad_token = tokenizer.\n",
    "\n",
    "print(tokenizer.model_max_length)  # Check the maximum length of the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb13a68",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "Choose some prompts. Then evaluate the model generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe65aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the finance instruction dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "# ds = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split=\"train[:5000]\")\n",
    "\n",
    "# Just read the first 5000 entries only due to resource limits\n",
    "# ds = load_dataset(\"talkmap/banking-conversation-corpus\", split=\"train[:5000]\")\n",
    "\n",
    "# ds = load_dataset(\"KidzRizal/twitter-sentiment-analysis\", split=\"train[:5000]\")\n",
    "\n",
    "dataset_name = \"AiresPucrs/sentiment-analysis\"\n",
    "ds = load_dataset(dataset_name, split=\"train[:5000]\")\n",
    "\n",
    "# split into train and test sets\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "# explore the dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29f1e",
   "metadata": {},
   "source": [
    "### Quickly check the model\n",
    "\n",
    "Use the first 10 texts from the test set to check how the model perform\n",
    "> The same set of prompts will be used before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a2dbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1c17b046-6667-45a8-86f3-55db3c0d4d97",
       "rows": [
        [
         "6",
         "i saw this movie when it aired on the wb and fell in love with riley smith immediately i would recommend the movie to people of all ages who just feel like being entertained and not much more i wish they'd air it again or cast riley smith in another movie!",
         "1"
        ],
        [
         "7",
         "cates is insipid and unconvincing kline over acts as always as does lithgow while butchering an english accent at least i assume that's what he's attempting and the tone staggers uneasily between farcical and maudlin as with most pet projects showcasing a celebrity couple it's a relief when this shoddy piece grinds to it's forced and jarring conclusion ",
         "0"
        ],
        [
         "15",
         "feels like an impressionistic film if there is such a thing the story is well told very poetic the characters well developed and well acted by the interpreters or interpreted by the actors the film delights in its own sumptuous emotions at times and works well unless you hate such emotion in movies not so in my case it's a very humanistic film the landscape and even the extraordinary situation of the displaced cook are very poetic in their own right well done a good classic for any good film collection ",
         "1"
        ],
        [
         "45",
         "the man who gave us splash cocoon and parenthood gave us this incoherent muddle of cliched characters poor plotting you've got to be kidding dialogue and melodramatic acting i guess everybody has a bad day at the office now and then he's allowed ",
         "0"
        ],
        [
         "46",
         "you know you're in trouble when the opening narration basically tells you who survives it all goes downhill from there unnecessary matrix influenced bullet time camera work pointless cuts to video game footage crusty old sea captains and wacky seamen ravers who become skilled combatants in the blink of an eye even the zombies are boring i was hoping for at least a so bad it's good zombie movie but this one is so bad those involved with its creation should be barred from ever making a movie again ",
         "0"
        ],
        [
         "49",
         "valley girl is the definitive 1980's movie with catch phrases filtered throughout this wonderfully acted movie the characters are so convincing that you forget it is a movie and not a video of an actual day in the life of any high school usa this flick is to the 1980's what the brady bunch tv series is to the 1970's if you don't like it well then gag me with a spoon ",
         "1"
        ],
        [
         "66",
         "to put it simply the fan was a disappointment it felt like as if i was watching taxi driver except taxi driver was much better than this it seemed like the filmmakers wanted us to root for robert deniro's character 100 percent this approach didn't work ",
         "0"
        ],
        [
         "72",
         "this film is really a big piece of trash trying to make itself look like a hollywood production poor story outline stupid robot story ultra bad acting by untalented pop idols and they are trying to fight !!!my goodness those miserable actors uses wires to make them look like they are good fighters and i hate that arrogant edison chen the worst actor i have ever seen!!!i will never touch his movies again avoid this movie at all costs!!!i wanted to give it a negative value out of ten not even worth a 0 10 ",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i saw this movie when it aired on the wb and f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cates is insipid and unconvincing kline over a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>feels like an impressionistic film if there is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>the man who gave us splash cocoon and parentho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>you know you're in trouble when the opening na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>valley girl is the definitive 1980's movie wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to put it simply the fan was a disappointment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>this film is really a big piece of trash tryin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "6   i saw this movie when it aired on the wb and f...      1\n",
       "7   cates is insipid and unconvincing kline over a...      0\n",
       "15  feels like an impressionistic film if there is...      1\n",
       "45  the man who gave us splash cocoon and parentho...      0\n",
       "46  you know you're in trouble when the opening na...      0\n",
       "49  valley girl is the definitive 1980's movie wit...      1\n",
       "66  to put it simply the fan was a disappointment ...      0\n",
       "72  this film is really a big piece of trash tryin...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df = ds['test'].to_pandas()\n",
    "check_df = check_df.loc[check_df['text'].str.len() < 512][:8]\n",
    "check_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30115109",
   "metadata": {},
   "source": [
    "Check the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f693d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.700641393661499}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6883817911148071}]\n",
      "[{'label': 'POSITIVE', 'score': 0.727775514125824}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6609954237937927}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6965090036392212}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6845595836639404}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6999365091323853}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6931561827659607}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orig_classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Quickly check the model\n",
    "for prompt in check_df['text'].tolist():\n",
    "    print(orig_classifier(prompt, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cca1ee",
   "metadata": {},
   "source": [
    "**Before training**, the model seems simply picks *POSITIVE*. This is expected according to the warning message. It needs to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941fa05",
   "metadata": {},
   "source": [
    "### Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d47ef7",
   "metadata": {},
   "source": [
    "#### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60940912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2287,  0.5141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# quick check that things are working\n",
    "\n",
    "inputs = tokenizer(ds['train'][0]['text'], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "inputs['input_ids'].shape\n",
    "#print(tokenizer.decode(inputs['input_ids']))\n",
    "outputs = model(**inputs)  # Forward pass with the tokenized inputs\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396449e",
   "metadata": {},
   "source": [
    "Define a function to group the *tokenized* text into smaller (block size 128) chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75727e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52eba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 9364.08 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9618.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Do the simple tokenization first and drop the un-used features.\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in ds.keys():\n",
    "    tokenized_datasets[split] = ds[split].map(\n",
    "        tokenize_func,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456d4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 4500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d7da1",
   "metadata": {},
   "source": [
    "### Setup PEFT for LORA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622b4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    #lora_alpha=32,\n",
    "    #lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91eb4c",
   "metadata": {},
   "source": [
    "#### Set the training Arguments and the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7be8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a compute metric function\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # Convert logits to predictions\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return { \"accuracy\": accuracy_score(y_true=labels, y_pred=predictions) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db1246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "save_path = \"./data/lora-finetuned-sentiment-analysis\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e47944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# let the data_collator handle the batching jobs\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fefa139",
   "metadata": {},
   "source": [
    "#### Evaluate the trainer **Before** training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee799bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangbing/pysrc/Udacity/genAIIntro/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 56:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7559791207313538, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.508, 'eval_runtime': 97.475, 'eval_samples_per_second': 5.13, 'eval_steps_per_second': 0.646}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "from transformers import pipeline\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a865c8",
   "metadata": {},
   "source": [
    "#### Now train the model. Without GPU, this will take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f32ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangbing/pysrc/Udacity/genAIIntro/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1126' max='1126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1126/1126 1:49:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.684114</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.679783</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangbing/pysrc/Udacity/genAIIntro/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1126, training_loss=0.6923937458763326, metrics={'train_runtime': 6558.9588, 'train_samples_per_second': 1.372, 'train_steps_per_second': 0.172, 'total_flos': 2109364641648960.0, 'train_loss': 0.6923937458763326, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8045c",
   "metadata": {},
   "source": [
    "#### Evaluate the trainer **After** training completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangbing/pysrc/Udacity/genAIIntro/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.679783284664154, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.606, 'eval_runtime': 94.2286, 'eval_samples_per_second': 5.306, 'eval_steps_per_second': 0.669, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "from transformers import pipeline\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f1c7e",
   "metadata": {},
   "source": [
    "#### Check the response for the same set of prompts after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58de7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.5392293334007263}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5339582562446594}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5598429441452026}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5076249837875366}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5189483165740967}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5159575939178467}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5249590873718262}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.5063350200653076}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "new_clfr = pipeline(\"sentiment-analysis\", model=lora_model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt in check_df['text'].tolist():\n",
    "    print(new_clfr(prompt, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  Save the PEFT Tuned model to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/lora-finetuned-sentiment-analysis/tokenizer_config.json',\n",
       " './data/lora-finetuned-sentiment-analysis/special_tokens_map.json',\n",
       " './data/lora-finetuned-sentiment-analysis/vocab.txt',\n",
       " './data/lora-finetuned-sentiment-analysis/added_tokens.json',\n",
       " './data/lora-finetuned-sentiment-analysis/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from peft import PeftConfig\n",
    "\n",
    "# save_path = \"./data/lora-finetuned-sentiment-analysis\" # (already defined above)\n",
    "lora_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a Saved PEFT Model\n",
    "\n",
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.5392293334007263}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5339582562446594}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5598429441452026}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5076249837875366}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5189483165740967}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5159575939178467}]\n",
      "[{'label': 'POSITIVE', 'score': 0.5249590873718262}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.5063350200653076}]\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned AutoPeftModelForSequenceClassification model for inference\n",
    "# save_path = \"./data/lora-finetuned-sentiment-analysis\" # (already defined above)\n",
    "# Load the fine-tuned model for inference\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "loaded_lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    save_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "new_clfr = pipeline(\"sentiment-analysis\", model=loaded_lora_model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt in check_df['text'].tolist():\n",
    "    print(new_clfr(prompt, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a8147",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The fine-tuned model does a better job in the area of finance related topics as the additional training\n",
    "dataset added more infomation to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d448e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
